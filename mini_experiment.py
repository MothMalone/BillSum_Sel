#!/usr/bin/env python3
"""
Quick Mini Experiment - LLaMA-2-7B Version
Tests the full pipeline with very small samples to verify everything works
"""

import os
import sys
import json
import logging
import torch
from datetime import datetime
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def clear_gpu_memory():
    """Clear GPU memory cache."""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def mini_experiment():
    """Run a mini version of the full experiment."""
    logger.info("üöÄ Starting Mini Experiment with LLaMA-2-7B")
    logger.info("=" * 60)
    
    # Import the main classes
    from real_production_experiment import (
        MemoryOptimizedPEFTTrainer, 
        ProductionEvaluator,
        load_dataset,
        random_selection,
        length_based_selection
    )
    
    try:
        # 1. Load tiny dataset
        logger.info("üìä Loading mini dataset...")
        train_dataset, test_dataset = load_dataset()
        
        # Use only 5 samples for training and 3 for evaluation
        mini_train = train_dataset.select(range(5))
        mini_test = test_dataset.select(range(3))
        
        logger.info(f"‚úÖ Mini train: {len(mini_train)} samples")
        logger.info(f"‚úÖ Mini test: {len(mini_test)} samples")
        
        # 2. Test data selection methods
        logger.info("üéØ Testing data selection...")
        
        # Random selection
        random_data = random_selection(mini_train, n_samples=3)
        logger.info(f"‚úÖ Random selection: {len(random_data)} samples")
        
        # Length-based selection  
        length_data = length_based_selection(mini_train, n_samples=3)
        logger.info(f"‚úÖ Length-based selection: {len(length_data)} samples")
        
        # 3. Test training (just setup, no actual training)
        logger.info("üèãÔ∏è  Testing PEFT trainer setup...")
        trainer = MemoryOptimizedPEFTTrainer()
        
        # Test trainer initialization
        success = trainer.setup_model_and_tokenizer()
        if success:
            logger.info("‚úÖ Trainer setup successful")
            
            # Clear model to save memory for evaluation test
            if hasattr(trainer, 'model'):
                del trainer.model
            if hasattr(trainer, 'tokenizer'):
                del trainer.tokenizer
            clear_gpu_memory()
        else:
            logger.error("‚ùå Trainer setup failed")
            return False
        
        # 4. Test evaluation metrics
        logger.info("üìà Testing evaluation metrics...")
        
        # Test basic evaluation functionality without full model setup
        sample_pred = "This is a test summary about a bill."
        sample_ref = "This is a reference summary about legislation."
        
        try:
            from rouge_score import rouge_scorer
            from bert_score import score
            
            # Test ROUGE
            rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
            rouge_scores = rouge.score(sample_ref, sample_pred)
            
            # Test BERTScore
            P, R, F1 = score([sample_pred], [sample_ref], lang="en", verbose=False)
            
            eval_result = {
                "rouge1": rouge_scores['rouge1'].fmeasure,
                "rouge2": rouge_scores['rouge2'].fmeasure,
                "rougeL": rouge_scores['rougeL'].fmeasure,
                "rougeLsum": rouge_scores['rougeL'].fmeasure,
                "bertscore_f1": F1.mean().item(),
                "bertscore_precision": P.mean().item(),
                "bertscore_recall": R.mean().item(),
                "num_samples": 1
            }
            
            logger.info(f"‚úÖ Evaluation metrics working")
            logger.info(f"   BERTScore F1: {eval_result['bertscore_f1']:.4f}")
            logger.info(f"   ROUGE-Lsum: {eval_result['rougeLsum']:.4f}")
            
        except Exception as e:
            logger.error(f"‚ùå Evaluation metrics failed: {e}")
            return False
        
        # 5. Save mini results
        results = {
            "experiment_type": "mini_test",
            "model": "meta-llama/Llama-2-7b-hf",
            "timestamp": datetime.now().isoformat(),
            "train_samples": len(mini_train),
            "test_samples": len(mini_test),
            "evaluation": eval_result,
            "status": "success"
        }
        
        with open("results/production/mini_experiment_results.json", 'w') as f:
            json.dump(results, f, indent=2)
        
        logger.info("üìÅ Mini results saved")
        
        # Clean up
        clear_gpu_memory()
        
        logger.info("=" * 60)
        logger.info("üéâ Mini experiment completed successfully!")
        logger.info("‚úÖ LLaMA-2-7B is working correctly")
        logger.info("‚úÖ Ready for full production experiment")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Mini experiment failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    # Ensure results directory exists
    Path("results/production").mkdir(parents=True, exist_ok=True)
    
    success = mini_experiment()
    sys.exit(0 if success else 1)
